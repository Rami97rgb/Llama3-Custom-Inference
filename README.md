# llama3-custom-inference
llama3 inference implementation with a custom flash attention decode kernel
